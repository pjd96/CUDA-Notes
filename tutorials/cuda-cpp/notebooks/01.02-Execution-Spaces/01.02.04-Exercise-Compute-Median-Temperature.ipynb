{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ynn1pjq5V42"
      },
      "source": [
        "## Exercise: Compute Median Temperature\n",
        "\n",
        "In many cases, porting code from CPU to GPU is as simple as replacing `std::` with `thrust::`.\n",
        "To verify this, we'll focus on calculating the median temperature in the vector.\n",
        "You'll start with a CPU-based implementation and modify the code to run on the GPU.\n",
        "Below is the original CPU code that you'll need to adapt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "GN7J6kwJ5V43",
        "outputId": "89f02688-4110-45da-e7e7-6d862bd94492",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-26 18:22:30 URL:https://raw.githubusercontent.com/NVIDIA/accelerated-computing-hub/refs/heads/main/tutorials/cuda-cpp/notebooks/01.02-Execution-Spaces/Sources/ach.h [2893/2893] -> \"Sources/ach.h\" [1]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "if os.getenv(\"COLAB_RELEASE_TAG\"): # If running in Google Colab:\n",
        "  !mkdir -p Sources\n",
        "  !wget https://raw.githubusercontent.com/NVIDIA/accelerated-computing-hub/refs/heads/main/tutorials/cuda-cpp/notebooks/01.02-Execution-Spaces/Sources/ach.h -nv -O Sources/ach.h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6dOZAKlJ5V44",
        "outputId": "4266902a-cd74-4419-da2f-df8b782229a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing Sources/port-sort-to-gpu.cpp\n"
          ]
        }
      ],
      "source": [
        "%%writefile Sources/port-sort-to-gpu.cpp\n",
        "#include \"ach.h\"\n",
        "\n",
        "float median(thrust::universal_vector<float> vec)\n",
        "{\n",
        "    // TODO: Make the code below execute on the GPU\n",
        "    thrust::sort(vec.begin(), vec.end());\n",
        "    return vec[vec.size() / 2];\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "    float k = 0.5;\n",
        "    float ambient_temp = 20;\n",
        "    thrust::universal_vector<float> temp{ 42, 24, 50 };\n",
        "    auto transformation = [=] __host__ __device__ (float temp) { return temp + k * (ambient_temp - temp); };\n",
        "\n",
        "    std::printf(\"step  median\\n\");\n",
        "    for (int step = 0; step < 3; step++) {\n",
        "        thrust::transform(thrust::device, temp.begin(), temp.end(), temp.begin(), transformation);\n",
        "        float median_temp = median(temp);\n",
        "        std::printf(\"%d     %.2f\\n\", step, median_temp);\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DSG4x6ve5V44",
        "outputId": "3211e87b-fb54-448e-ae9b-590d475790c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step  median\n",
            "0     31.00\n",
            "1     25.50\n",
            "2     22.75\n"
          ]
        }
      ],
      "source": [
        "!nvcc -o /tmp/a.out --extended-lambda Sources/port-sort-to-gpu.cpp -x cu -arch=native # build executable\n",
        "!/tmp/a.out # run executable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCEd92h65V44"
      },
      "source": [
        "If everything goes well, the cell above should print:\n",
        "\n",
        "| step | median\n",
        "| :--- | :-----\n",
        "| 0    | 31.00\n",
        "| 1    | 25.50\n",
        "| 2    | 22.75"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26uSIESr5V45"
      },
      "source": [
        "If you’re unsure how to proceed, consider expanding this section for guidance. Use the hint only after giving the problem a genuine attempt.\n",
        "\n",
        "<details>\n",
        "  <summary>Hints</summary>\n",
        "  \n",
        "  - Thrust provides a `thrust::sort` function that can be used to sort data on the GPU\n",
        "  - Use `thrust::device` execution policy to specify where you want `thrust::sort` to run\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0I4s2Fgs5V45"
      },
      "source": [
        "Open this section only after you’ve made a serious attempt at solving the problem. Once you’ve completed your solution, compare it with the reference provided here to evaluate your approach and identify any potential improvements.\n",
        "\n",
        "<details>\n",
        "  <summary>Solution</summary>\n",
        "\n",
        "  Key points:\n",
        "  - change `std::sort` to `thrust::sort`\n",
        "  - add `thrust::device` execution policy parameter\n",
        "\n",
        "  Solution:\n",
        "  ```c++\n",
        "  float median(thrust::universal_vector<float> vec) {\n",
        "    thrust::sort(thrust::device, vec.begin(), vec.end());\n",
        "    return vec[vec.size() / 2];\n",
        "  }\n",
        "  ```\n",
        "\n",
        "  You can find full solution [here](Solutions/port-sort-to-gpu.cpp).\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akTM6Q5t5V45"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "Now you should be comfortable with the concept of execution spaces and be able to port standard algorithms to GPU with Thrust!\n",
        "\n",
        "But what if your algorithm is not a standard one?\n",
        "\n",
        "Proceed to the [next section](../01.03-Extending-Algorithms/01.03.01-Extending-Algorithms.ipynb) to learn how to extend standard algorithms to your unique use cases."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}